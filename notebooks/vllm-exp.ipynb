{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "from vllm import EngineArgs, LLMEngine, RequestOutput, SamplingParams\n",
    "from vllm.lora.request import LoRARequest\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "WANDB_API_KEY = os.getenv(\"WANDB_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_prompts(\n",
    "        sql_lora_path: str,\n",
    "        oasst_lora_path: str\n",
    ") -> List[Tuple[str, SamplingParams, Optional[LoRARequest]]]:\n",
    "    \"\"\"Create a list of test prompts with their sampling parameters.\n",
    "\n",
    "    2 requests for base model, 4 requests for the LoRA. We define 2\n",
    "    different LoRA adapters (using the same model for demo purposes).\n",
    "    Since we also set `max_loras=1`, the expectation is that the requests\n",
    "    with the second LoRA adapter will be ran after all requests with the\n",
    "    first adapter have finished.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        (\"A robot may not injure a human being\",\n",
    "         SamplingParams(temperature=0.0,\n",
    "                        logprobs=1,\n",
    "                        prompt_logprobs=1,\n",
    "                        max_tokens=128), None),\n",
    "        (\"To be or not to be,\",\n",
    "         SamplingParams(temperature=0.8,\n",
    "                        top_k=5,\n",
    "                        presence_penalty=0.2,\n",
    "                        max_tokens=128), None),\n",
    "        (\n",
    "            \"[user] Write a SQL query to answer the question based on the table schema.\\n\\n context: CREATE TABLE table_name_74 (icao VARCHAR, airport VARCHAR)\\n\\n question: Name the ICAO for lilongwe international airport [/user] [assistant]\",  # noqa: E501\n",
    "            SamplingParams(temperature=0.0,\n",
    "                           logprobs=1,\n",
    "                           prompt_logprobs=1,\n",
    "                           max_tokens=128,\n",
    "                           stop_token_ids=[32003]),\n",
    "            LoRARequest(\"sql-lora\", 1, sql_lora_path)),\n",
    "        (\n",
    "            \"[user] Write a SQL query to answer the question based on the table schema.\\n\\n context: CREATE TABLE table_name_11 (nationality VARCHAR, elector VARCHAR)\\n\\n question: When Anchero Pantaleone was the elector what is under nationality? [/user] [assistant]\",  # noqa: E501\n",
    "            SamplingParams(n=3,\n",
    "                           best_of=3,\n",
    "                           use_beam_search=True,\n",
    "                           temperature=0,\n",
    "                           max_tokens=128,\n",
    "                           stop_token_ids=[32003]),\n",
    "            LoRARequest(\"sql-lora\", 1, sql_lora_path)),\n",
    "        (\n",
    "            \"[user] Write a SQL query to answer the question based on the table schema.\\n\\n context: CREATE TABLE table_name_74 (icao VARCHAR, airport VARCHAR)\\n\\n question: Name the ICAO for lilongwe international airport [/user] [assistant]\",  # noqa: E501\n",
    "            SamplingParams(temperature=0.0,\n",
    "                           logprobs=1,\n",
    "                           prompt_logprobs=1,\n",
    "                           max_tokens=128,\n",
    "                           stop_token_ids=[32003]),\n",
    "            LoRARequest(\"sql-lora2\", 2, sql_lora_path)),\n",
    "        (\n",
    "            \"[user] Write a SQL query to answer the question based on the table schema.\\n\\n context: CREATE TABLE table_name_11 (nationality VARCHAR, elector VARCHAR)\\n\\n question: When Anchero Pantaleone was the elector what is under nationality? [/user] [assistant]\",  # noqa: E501\n",
    "            SamplingParams(n=3,\n",
    "                           best_of=3,\n",
    "                           use_beam_search=True,\n",
    "                           temperature=0,\n",
    "                           max_tokens=128,\n",
    "                           stop_token_ids=[32003]),\n",
    "            LoRARequest(\"sql-lora\", 1, sql_lora_path)),\n",
    "        (\n",
    "            \"[user] Can you explain the difference between a river and a riverboat? [/user] [assistant]\",  # noqa: E501\n",
    "            SamplingParams(temperature=0.0,\n",
    "                           logprobs=1,\n",
    "                           prompt_logprobs=1,\n",
    "                           max_tokens=128,\n",
    "                           stop_token_ids=[32003]),\n",
    "            LoRARequest(\"oasst-lora\", 1, oasst_lora_path)),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_requests(engine: LLMEngine,\n",
    "                     test_prompts: List[Tuple[str, SamplingParams,\n",
    "                                              Optional[LoRARequest]]]):\n",
    "    \"\"\"Continuously process a list of prompts and handle the outputs.\"\"\"\n",
    "    request_id = 0\n",
    "\n",
    "    while test_prompts or engine.has_unfinished_requests():\n",
    "        if test_prompts:\n",
    "            prompt, sampling_params, lora_request = test_prompts.pop(0)\n",
    "            engine.add_request(str(request_id),\n",
    "                               prompt,\n",
    "                               sampling_params,\n",
    "                               lora_request=lora_request)\n",
    "            request_id += 1\n",
    "\n",
    "        request_outputs: List[RequestOutput] = engine.step()\n",
    "\n",
    "        for request_output in request_outputs:\n",
    "            if request_output.finished:\n",
    "                print(request_output)\n",
    "    return request_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_engine() -> LLMEngine:\n",
    "    \"\"\"Initialize the LLMEngine.\"\"\"\n",
    "    # max_loras: controls the number of LoRAs that can be used in the same\n",
    "    #   batch. Larger numbers will cause higher memory usage, as each LoRA\n",
    "    #   slot requires its own preallocated tensor.\n",
    "    # max_lora_rank: controls the maximum supported rank of all LoRAs. Larger\n",
    "    #   numbers will cause higher memory usage. If you know that all LoRAs will\n",
    "    #   use the same rank, it is recommended to set this as low as possible.\n",
    "    # max_cpu_loras: controls the size of the CPU LoRA cache.\n",
    "    engine_args = EngineArgs(model=\"meta-llama/Llama-2-7b-hf\",\n",
    "                             enable_lora=True,\n",
    "                             max_loras=2,\n",
    "                             max_lora_rank=8,\n",
    "                             max_cpu_loras=2,\n",
    "                             max_num_seqs=256,\n",
    "                             tensor_parallel_size=1,)\n",
    "    return LLMEngine.from_engine_args(engine_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    \"\"\"Main function that sets up and runs the prompt processing.\"\"\"\n",
    "    engine = initialize_engine()\n",
    "    sql_lora_path = snapshot_download(repo_id=\"yard1/llama-2-7b-sql-lora-test\")\n",
    "    oasst_lora_path = snapshot_download(repo_id=\"kaitchup/Llama-2-7B-oasstguanaco-adapter-1e\")\n",
    "    test_prompts = create_test_prompts(sql_lora_path, oasst_lora_path)\n",
    "    output = process_requests(engine, test_prompts)\n",
    "    return output\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-02 13:50:38 llm_engine.py:161] Initializing an LLM engine (v0.4.3) with config: model='meta-llama/Llama-2-7b-hf', speculative_config=None, tokenizer='meta-llama/Llama-2-7b-hf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=meta-llama/Llama-2-7b-hf)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harpreet_guest2/akshat/Multi-Qlora/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-02 13:50:38 weight_utils.py:207] Using model weights format ['*.safetensors']\n",
      "INFO 06-02 13:50:40 model_runner.py:146] Loading model weights took 12.5581 GB\n",
      "INFO 06-02 13:50:42 gpu_executor.py:83] # GPU blocks: 3781, # CPU blocks: 512\n",
      "INFO 06-02 13:50:44 model_runner.py:854] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 06-02 13:50:44 model_runner.py:858] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 06-02 13:50:47 model_runner.py:924] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d957d2880f44bf5b9e9e9b6a11983dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d11b62477eea4454af78e36ad28c37eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RequestOutput(request_id=6, prompt='[user] Can you explain the difference between a river and a riverboat? [/user] [assistant]', prompt_token_ids=[1, 32000, 29871, 1815, 366, 5649, 278, 4328, 1546, 263, 8580, 322, 263, 8580, 833, 271, 29973, 29871, 32001, 259, 32002], prompt_logprobs=[None, {32000: Logprob(logprob=-0.004840085748583078, rank=1, decoded_token='[user]')}, {29871: Logprob(logprob=-0.1254420429468155, rank=1, decoded_token='')}, {1815: Logprob(logprob=-15.57059383392334, rank=179, decoded_token='Can'), 14350: Logprob(logprob=-0.0002809368306770921, rank=1, decoded_token='Write')}, {366: Logprob(logprob=-1.1787023544311523, rank=1, decoded_token='you')}, {5649: Logprob(logprob=-3.265174388885498, rank=4, decoded_token='explain'), 2649: Logprob(logprob=-1.858924388885498, rank=1, decoded_token='tell')}, {278: Logprob(logprob=-1.4558932781219482, rank=1, decoded_token='the')}, {4328: Logprob(logprob=-2.698925018310547, rank=1, decoded_token='difference')}, {1546: Logprob(logprob=-0.14731164276599884, rank=1, decoded_token='between')}, {263: Logprob(logprob=-1.9995191097259521, rank=2, decoded_token='a'), 278: Logprob(logprob=-1.5698316097259521, rank=1, decoded_token='the')}, {8580: Logprob(logprob=-9.750932693481445, rank=2195, decoded_token='river'), 421: Logprob(logprob=-2.737260341644287, rank=1, decoded_token='`')}, {322: Logprob(logprob=-0.7846433520317078, rank=1, decoded_token='and')}, {263: Logprob(logprob=-0.13374431431293488, rank=1, decoded_token='a')}, {8580: Logprob(logprob=-4.120709419250488, rank=7, decoded_token='river'), 4840: Logprob(logprob=-0.7300844788551331, rank=1, decoded_token='stream')}, {833: Logprob(logprob=-6.473799705505371, rank=40, decoded_token='bo'), 2580: Logprob(logprob=-1.5987998247146606, rank=1, decoded_token='bed')}, {271: Logprob(logprob=-0.0006899837171658874, rank=1, decoded_token='at')}, {29973: Logprob(logprob=-0.2190292477607727, rank=1, decoded_token='?')}, {29871: Logprob(logprob=-2.416841745376587, rank=2, decoded_token=''), 13: Logprob(logprob=-0.3621543049812317, rank=1, decoded_token='\\n')}, {32001: Logprob(logprob=-7.354693412780762, rank=100, decoded_token='[/user]'), 14350: Logprob(logprob=-1.6085995435714722, rank=1, decoded_token='Write')}, {259: Logprob(logprob=-1.7294896841049194, rank=2, decoded_token=' '), 13: Logprob(logprob=-1.0263646841049194, rank=1, decoded_token='\\n')}, {32002: Logprob(logprob=-0.47880566120147705, rank=1, decoded_token='[assistant]')}], outputs=[CompletionOutput(index=0, text='  [assistant]  Can you explain the difference between a river and a riverboat? ', token_ids=[259, 32002, 29871, 1815, 366, 5649, 278, 4328, 1546, 263, 8580, 322, 263, 8580, 833, 271, 29973, 29871, 32003], cumulative_logprob=-5.635791915934533, logprobs=[{259: Logprob(logprob=-0.8350605368614197, rank=1, decoded_token='  ')}, {32002: Logprob(logprob=-0.7887685894966125, rank=1, decoded_token='[assistant]')}, {29871: Logprob(logprob=-0.44780099391937256, rank=1, decoded_token=' ')}, {1815: Logprob(logprob=-2.138850450515747, rank=1, decoded_token=' Can')}, {366: Logprob(logprob=-0.06497523188591003, rank=1, decoded_token=' you')}, {5649: Logprob(logprob=-0.24072331190109253, rank=1, decoded_token=' explain')}, {278: Logprob(logprob=-0.14893491566181183, rank=1, decoded_token=' the')}, {4328: Logprob(logprob=-0.11038564145565033, rank=1, decoded_token=' difference')}, {1546: Logprob(logprob=-0.02716013975441456, rank=1, decoded_token=' between')}, {263: Logprob(logprob=-0.22384120523929596, rank=1, decoded_token=' a')}, {8580: Logprob(logprob=-0.2743375301361084, rank=1, decoded_token=' river')}, {322: Logprob(logprob=-0.10860645025968552, rank=1, decoded_token=' and')}, {263: Logprob(logprob=-0.026642579585313797, rank=1, decoded_token=' a')}, {8580: Logprob(logprob=-0.038141824305057526, rank=1, decoded_token=' river')}, {833: Logprob(logprob=-0.03217044100165367, rank=1, decoded_token='bo')}, {271: Logprob(logprob=-0.0001525762490928173, rank=1, decoded_token='at')}, {29973: Logprob(logprob=-0.011052930727601051, rank=1, decoded_token='?')}, {29871: Logprob(logprob=-0.054479826241731644, rank=1, decoded_token=' ')}, {32003: Logprob(logprob=-0.06370674073696136, rank=1, decoded_token='[/assistant]')}], finish_reason=stop, stop_reason=32003)], finished=True, metrics=RequestMetrics(arrival_time=1717354248.5932634, last_token_time=1717354249.0993915, first_scheduled_time=1717354248.5935357, first_token_time=1717354248.6208966, time_in_queue=0.0002722740173339844, finished_time=1717354249.0993836), lora_request=LoRARequest(lora_name='oasst-lora', lora_int_id=1, lora_local_path='/home/harpreet_guest2/.cache/huggingface/hub/models--kaitchup--Llama-2-7B-oasstguanaco-adapter-1e/snapshots/b156bd3823c73832fe14e97355216898cd9e6380', long_lora_max_len=None))\n",
      "RequestOutput(request_id=2, prompt='[user] Write a SQL query to answer the question based on the table schema.\\n\\n context: CREATE TABLE table_name_74 (icao VARCHAR, airport VARCHAR)\\n\\n question: Name the ICAO for lilongwe international airport [/user] [assistant]', prompt_token_ids=[1, 32000, 29871, 14350, 263, 3758, 2346, 304, 1234, 278, 1139, 2729, 373, 278, 1591, 10938, 29889, 13, 13, 3030, 29901, 14602, 10911, 1591, 29918, 978, 29918, 29955, 29946, 313, 983, 29877, 21748, 29892, 4799, 637, 21748, 29897, 13, 13, 1139, 29901, 4408, 278, 306, 5454, 29949, 363, 301, 309, 549, 705, 6121, 4799, 637, 29871, 32001, 259, 32002], prompt_logprobs=[None, {32000: Logprob(logprob=-0.004773648921400309, rank=1, decoded_token='[user]')}, {29871: Logprob(logprob=-0.12253902107477188, rank=1, decoded_token='')}, {14350: Logprob(logprob=-0.0002803409588523209, rank=1, decoded_token='Write')}, {263: Logprob(logprob=-0.0004917366313748062, rank=1, decoded_token='a')}, {3758: Logprob(logprob=-0.0004319211875554174, rank=1, decoded_token='SQL')}, {2346: Logprob(logprob=-0.0005189026123844087, rank=1, decoded_token='query')}, {304: Logprob(logprob=-0.00041940953815355897, rank=1, decoded_token='to')}, {1234: Logprob(logprob=-0.00020001317898277193, rank=1, decoded_token='answer')}, {278: Logprob(logprob=-0.00045658653834834695, rank=1, decoded_token='the')}, {1139: Logprob(logprob=-0.0006170752458274364, rank=1, decoded_token='question')}, {2729: Logprob(logprob=-0.0004844683862756938, rank=1, decoded_token='based')}, {373: Logprob(logprob=-0.0006945105269551277, rank=1, decoded_token='on')}, {278: Logprob(logprob=-2.3007127310847864e-05, rank=1, decoded_token='the')}, {1591: Logprob(logprob=-0.00047672350774519145, rank=1, decoded_token='table')}, {10938: Logprob(logprob=-0.0010026433737948537, rank=1, decoded_token='schema')}, {29889: Logprob(logprob=-0.0014340127818286419, rank=1, decoded_token='.')}, {13: Logprob(logprob=-0.004083035048097372, rank=1, decoded_token='\\n')}, {13: Logprob(logprob=-0.000774798565544188, rank=1, decoded_token='\\n')}, {3030: Logprob(logprob=-0.001105412608012557, rank=1, decoded_token=' context')}, {29901: Logprob(logprob=-3.349725011503324e-05, rank=1, decoded_token='\\n:')}, {14602: Logprob(logprob=-0.00024256148026324809, rank=1, decoded_token='\\n\\n CREATE')}, {10911: Logprob(logprob=-0.0005957497633062303, rank=1, decoded_token='\\n\\n\\n TABLE')}, {1591: Logprob(logprob=-0.07258503884077072, rank=1, decoded_token='\\n\\n\\n\\n table')}, {29918: Logprob(logprob=-0.0006317288498394191, rank=1, decoded_token='\\n\\n\\n\\n\\n_')}, {978: Logprob(logprob=-0.3755280077457428, rank=1, decoded_token='\\n\\n\\n\\n\\n\\nname')}, {29918: Logprob(logprob=-0.0005237876321189106, rank=1, decoded_token='\\n\\n\\n\\n\\n\\n\\n_')}, {29955: Logprob(logprob=-2.1367220878601074, rank=3, decoded_token='\\n\\n\\n\\n\\n\\n\\n\\n7'), 29953: Logprob(logprob=-2.1054720878601074, rank=1, decoded_token='\\n\\n\\n\\n\\n\\n\\n\\n6')}, {29946: Logprob(logprob=-2.3930678367614746, rank=6, decoded_token='\\n\\n\\n\\n\\n\\n\\n\\n\\n4'), 29896: Logprob(logprob=-2.2368178367614746, rank=1, decoded_token='\\n\\n\\n\\n\\n\\n\\n\\n\\n1')}, {313: Logprob(logprob=-0.0012511529494076967, rank=1, decoded_token='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n (')}, {983: Logprob(logprob=-11.944225311279297, rank=3308, decoded_token='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nica'), 1256: Logprob(logprob=-3.170788288116455, rank=1, decoded_token='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\ndate')}, {29877: Logprob(logprob=-0.08102887123823166, rank=1, decoded_token='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\no')}, {21748: Logprob(logprob=-0.6854418516159058, rank=1, decoded_token='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n VARCHAR')}, {29892: Logprob(logprob=-0.005799727980047464, rank=1, decoded_token='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n,')}, {4799: Logprob(logprob=-2.1781036853790283, rank=1, decoded_token='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n air')}, {637: Logprob(logprob=-1.936806321144104, rank=2, decoded_token='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nport'), 1220: Logprob(logprob=-0.2414938360452652, rank=1, decoded_token='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nline')}, {21748: Logprob(logprob=-0.4293473958969116, rank=1, decoded_token='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n VARCHAR')}, {29897: Logprob(logprob=-0.1584867238998413, rank=1, decoded_token='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n)')}, {13: Logprob(logprob=-0.002474462613463402, rank=1, decoded_token='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')}, {13: Logprob(logprob=-0.00011824862303910777, rank=1, decoded_token='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')}, {1139: Logprob(logprob=-7.73638384998776e-05, rank=1, decoded_token=' question')}, {29901: Logprob(logprob=-0.00013219437096267939, rank=1, decoded_token='\\n:')}, {4408: Logprob(logprob=-2.1940650939941406, rank=3, decoded_token='\\n\\n Name'), 1724: Logprob(logprob=-0.6471899747848511, rank=1, decoded_token='\\n\\n What')}, {278: Logprob(logprob=-0.21818111836910248, rank=1, decoded_token='\\n\\n\\n the')}, {306: Logprob(logprob=-0.9201486110687256, rank=2, decoded_token='\\n\\n\\n\\n I'), 29871: Logprob(logprob=-0.8810861110687256, rank=1, decoded_token='\\n\\n\\n\\n ')}, {5454: Logprob(logprob=-0.07014685124158859, rank=1, decoded_token='\\n\\n\\n\\n\\nCA')}, {29949: Logprob(logprob=-0.0022903657518327236, rank=1, decoded_token='\\n\\n\\n\\n\\n\\nO')}, {363: Logprob(logprob=-1.1083241701126099, rank=2, decoded_token='\\n\\n\\n\\n\\n\\n\\n for'), 310: Logprob(logprob=-1.1005116701126099, rank=1, decoded_token='\\n\\n\\n\\n\\n\\n\\n of')}, {301: Logprob(logprob=-6.23159122467041, rank=51, decoded_token='\\n\\n\\n\\n\\n\\n\\n\\n l'), 278: Logprob(logprob=-1.204247236251831, rank=1, decoded_token='\\n\\n\\n\\n\\n\\n\\n\\n the')}, {309: Logprob(logprob=-4.745606899261475, rank=20, decoded_token='\\n\\n\\n\\n\\n\\n\\n\\n\\nil'), 898: Logprob(logprob=-2.1362318992614746, rank=1, decoded_token='\\n\\n\\n\\n\\n\\n\\n\\n\\nond')}, {549: Logprob(logprob=-0.7052618265151978, rank=1, decoded_token='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nong')}, {705: Logprob(logprob=-0.002821395406499505, rank=1, decoded_token='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nwe')}, {6121: Logprob(logprob=-1.5109386444091797, rank=2, decoded_token='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n international'), 4799: Logprob(logprob=-1.2375011444091797, rank=1, decoded_token='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n air')}, {4799: Logprob(logprob=-0.1420583575963974, rank=1, decoded_token='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n air')}, {637: Logprob(logprob=-0.0024986020289361477, rank=1, decoded_token='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nport')}, {29871: Logprob(logprob=-0.4706132113933563, rank=1, decoded_token='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n ')}, {32001: Logprob(logprob=-0.005849386565387249, rank=1, decoded_token='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[/user]')}, {259: Logprob(logprob=-0.0001110968878492713, rank=1, decoded_token='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  ')}, {32002: Logprob(logprob=-0.0002335037279408425, rank=1, decoded_token='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[assistant]')}], outputs=[CompletionOutput(index=0, text=\"  SELECT icao FROM table_name_74 WHERE airport = 'lilongwe international airport' \", token_ids=[29871, 5097, 474, 1113, 29877, 3895, 1591, 29918, 978, 29918, 29955, 29946, 5754, 4799, 637, 353, 525, 29880, 309, 549, 705, 6121, 4799, 637, 29915, 29871, 32003], cumulative_logprob=-1.4579286568549605, logprobs=[{29871: Logprob(logprob=-1.6093124941107817e-05, rank=1, decoded_token=' ')}, {5097: Logprob(logprob=-0.0011731653939932585, rank=1, decoded_token=' SELECT')}, {474: Logprob(logprob=-1.072112798690796, rank=1, decoded_token=' i')}, {1113: Logprob(logprob=-0.08755894005298615, rank=1, decoded_token='ca')}, {29877: Logprob(logprob=-1.3351351299206726e-05, rank=1, decoded_token='o')}, {3895: Logprob(logprob=-0.00622536102309823, rank=1, decoded_token=' FROM')}, {1591: Logprob(logprob=-0.0008118432597257197, rank=1, decoded_token=' table')}, {29918: Logprob(logprob=-1.4305104514278355e-06, rank=1, decoded_token='_')}, {978: Logprob(logprob=-9.131014667218551e-05, rank=1, decoded_token='name')}, {29918: Logprob(logprob=-6.437280717364047e-06, rank=1, decoded_token='_')}, {29955: Logprob(logprob=-0.0001389883691444993, rank=1, decoded_token='7')}, {29946: Logprob(logprob=-8.642300235806033e-05, rank=1, decoded_token='4')}, {5754: Logprob(logprob=-0.0008311392739415169, rank=1, decoded_token=' WHERE')}, {4799: Logprob(logprob=-0.0040996563620865345, rank=1, decoded_token=' air')}, {637: Logprob(logprob=-4.172238186583854e-05, rank=1, decoded_token='port')}, {353: Logprob(logprob=-0.0031932813581079245, rank=1, decoded_token=' =')}, {525: Logprob(logprob=-0.0027977393474429846, rank=1, decoded_token=\" '\")}, {29880: Logprob(logprob=-0.1968197226524353, rank=1, decoded_token='l')}, {309: Logprob(logprob=-0.0003352795320097357, rank=1, decoded_token='il')}, {549: Logprob(logprob=-4.8636207793606445e-05, rank=1, decoded_token='ong')}, {705: Logprob(logprob=-0.0003909300430677831, rank=1, decoded_token='we')}, {6121: Logprob(logprob=-0.0031704658176749945, rank=1, decoded_token=' international')}, {4799: Logprob(logprob=-0.07165490835905075, rank=1, decoded_token=' air')}, {637: Logprob(logprob=-0.0001012035645544529, rank=1, decoded_token='port')}, {29915: Logprob(logprob=-0.00488386070355773, rank=1, decoded_token=\"'\")}, {29871: Logprob(logprob=-0.0005735897575505078, rank=1, decoded_token=' ')}, {32003: Logprob(logprob=-0.0007503792876377702, rank=1, decoded_token='[/assistant]')}], finish_reason=stop, stop_reason=32003)], finished=True, metrics=RequestMetrics(arrival_time=1717354248.2307532, last_token_time=1717354249.314395, first_scheduled_time=1717354248.2885847, first_token_time=1717354248.3637974, time_in_queue=0.057831525802612305, finished_time=1717354249.314359), lora_request=LoRARequest(lora_name='sql-lora', lora_int_id=1, lora_local_path='/home/harpreet_guest2/.cache/huggingface/hub/models--yard1--llama-2-7b-sql-lora-test/snapshots/0dfa347e8877a4d4ed19ee56c140fa518470028c', long_lora_max_len=None))\n",
      "RequestOutput(request_id=4, prompt='[user] Write a SQL query to answer the question based on the table schema.\\n\\n context: CREATE TABLE table_name_74 (icao VARCHAR, airport VARCHAR)\\n\\n question: Name the ICAO for lilongwe international airport [/user] [assistant]', prompt_token_ids=[1, 32000, 29871, 14350, 263, 3758, 2346, 304, 1234, 278, 1139, 2729, 373, 278, 1591, 10938, 29889, 13, 13, 3030, 29901, 14602, 10911, 1591, 29918, 978, 29918, 29955, 29946, 313, 983, 29877, 21748, 29892, 4799, 637, 21748, 29897, 13, 13, 1139, 29901, 4408, 278, 306, 5454, 29949, 363, 301, 309, 549, 705, 6121, 4799, 637, 29871, 32001, 259, 32002], prompt_logprobs=[None, {32000: Logprob(logprob=-0.004773648921400309, rank=1, decoded_token='[user]')}, {29871: Logprob(logprob=-0.12253902107477188, rank=1, decoded_token='')}, {14350: Logprob(logprob=-0.0002803409588523209, rank=1, decoded_token='Write')}, {263: Logprob(logprob=-0.0004917366313748062, rank=1, decoded_token='a')}, {3758: Logprob(logprob=-0.0004319211875554174, rank=1, decoded_token='SQL')}, {2346: Logprob(logprob=-0.0005189026123844087, rank=1, decoded_token='query')}, {304: Logprob(logprob=-0.00041940953815355897, rank=1, decoded_token='to')}, {1234: Logprob(logprob=-0.00020001317898277193, rank=1, decoded_token='answer')}, {278: Logprob(logprob=-0.00045658653834834695, rank=1, decoded_token='the')}, {1139: Logprob(logprob=-0.0006170752458274364, rank=1, decoded_token='question')}, {2729: Logprob(logprob=-0.0004844683862756938, rank=1, decoded_token='based')}, {373: Logprob(logprob=-0.0006945105269551277, rank=1, decoded_token='on')}, {278: Logprob(logprob=-2.3007127310847864e-05, rank=1, decoded_token='the')}, {1591: Logprob(logprob=-0.00047672350774519145, rank=1, decoded_token='table')}, {10938: Logprob(logprob=-0.0010026433737948537, rank=1, decoded_token='schema')}, {29889: Logprob(logprob=-0.0014340127818286419, rank=1, decoded_token='.')}, {13: Logprob(logprob=-0.004083035048097372, rank=1, decoded_token='\\n')}, {13: Logprob(logprob=-0.000774798565544188, rank=1, decoded_token='\\n')}, {3030: Logprob(logprob=-0.001105412608012557, rank=1, decoded_token=' context')}, {29901: Logprob(logprob=-3.349725011503324e-05, rank=1, decoded_token='\\n:')}, {14602: Logprob(logprob=-0.00024256148026324809, rank=1, decoded_token='\\n\\n CREATE')}, {10911: Logprob(logprob=-0.0005957497633062303, rank=1, decoded_token='\\n\\n\\n TABLE')}, {1591: Logprob(logprob=-0.07258503884077072, rank=1, decoded_token='\\n\\n\\n\\n table')}, {29918: Logprob(logprob=-0.0006317288498394191, rank=1, decoded_token='\\n\\n\\n\\n\\n_')}, {978: Logprob(logprob=-0.3755280077457428, rank=1, decoded_token='\\n\\n\\n\\n\\n\\nname')}, {29918: Logprob(logprob=-0.0005237876321189106, rank=1, decoded_token='\\n\\n\\n\\n\\n\\n\\n_')}, {29955: Logprob(logprob=-2.1367220878601074, rank=3, decoded_token='\\n\\n\\n\\n\\n\\n\\n\\n7'), 29953: Logprob(logprob=-2.1054720878601074, rank=1, decoded_token='\\n\\n\\n\\n\\n\\n\\n\\n6')}, {29946: Logprob(logprob=-2.3930678367614746, rank=6, decoded_token='\\n\\n\\n\\n\\n\\n\\n\\n\\n4'), 29896: Logprob(logprob=-2.2368178367614746, rank=1, decoded_token='\\n\\n\\n\\n\\n\\n\\n\\n\\n1')}, {313: Logprob(logprob=-0.0012511529494076967, rank=1, decoded_token='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n (')}, {983: Logprob(logprob=-11.944225311279297, rank=3308, decoded_token='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nica'), 1256: Logprob(logprob=-3.170788288116455, rank=1, decoded_token='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\ndate')}, {29877: Logprob(logprob=-0.08102887123823166, rank=1, decoded_token='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\no')}, {21748: Logprob(logprob=-0.6854418516159058, rank=1, decoded_token='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n VARCHAR')}, {29892: Logprob(logprob=-0.005799727980047464, rank=1, decoded_token='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n,')}, {4799: Logprob(logprob=-2.1781036853790283, rank=1, decoded_token='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n air')}, {637: Logprob(logprob=-1.936806321144104, rank=2, decoded_token='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nport'), 1220: Logprob(logprob=-0.2414938360452652, rank=1, decoded_token='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nline')}, {21748: Logprob(logprob=-0.4293473958969116, rank=1, decoded_token='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n VARCHAR')}, {29897: Logprob(logprob=-0.1584867238998413, rank=1, decoded_token='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n)')}, {13: Logprob(logprob=-0.002474462613463402, rank=1, decoded_token='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')}, {13: Logprob(logprob=-0.00011824862303910777, rank=1, decoded_token='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')}, {1139: Logprob(logprob=-7.73638384998776e-05, rank=1, decoded_token=' question')}, {29901: Logprob(logprob=-0.00013219437096267939, rank=1, decoded_token='\\n:')}, {4408: Logprob(logprob=-2.1940650939941406, rank=3, decoded_token='\\n\\n Name'), 1724: Logprob(logprob=-0.6471899747848511, rank=1, decoded_token='\\n\\n What')}, {278: Logprob(logprob=-0.21818111836910248, rank=1, decoded_token='\\n\\n\\n the')}, {306: Logprob(logprob=-0.9201486110687256, rank=2, decoded_token='\\n\\n\\n\\n I'), 29871: Logprob(logprob=-0.8810861110687256, rank=1, decoded_token='\\n\\n\\n\\n ')}, {5454: Logprob(logprob=-0.07014685124158859, rank=1, decoded_token='\\n\\n\\n\\n\\nCA')}, {29949: Logprob(logprob=-0.0022903657518327236, rank=1, decoded_token='\\n\\n\\n\\n\\n\\nO')}, {363: Logprob(logprob=-1.1083241701126099, rank=2, decoded_token='\\n\\n\\n\\n\\n\\n\\n for'), 310: Logprob(logprob=-1.1005116701126099, rank=1, decoded_token='\\n\\n\\n\\n\\n\\n\\n of')}, {301: Logprob(logprob=-6.23159122467041, rank=51, decoded_token='\\n\\n\\n\\n\\n\\n\\n\\n l'), 278: Logprob(logprob=-1.204247236251831, rank=1, decoded_token='\\n\\n\\n\\n\\n\\n\\n\\n the')}, {309: Logprob(logprob=-4.745606899261475, rank=20, decoded_token='\\n\\n\\n\\n\\n\\n\\n\\n\\nil'), 898: Logprob(logprob=-2.1362318992614746, rank=1, decoded_token='\\n\\n\\n\\n\\n\\n\\n\\n\\nond')}, {549: Logprob(logprob=-0.7052618265151978, rank=1, decoded_token='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nong')}, {705: Logprob(logprob=-0.002821395406499505, rank=1, decoded_token='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nwe')}, {6121: Logprob(logprob=-1.5109386444091797, rank=2, decoded_token='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n international'), 4799: Logprob(logprob=-1.2375011444091797, rank=1, decoded_token='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n air')}, {4799: Logprob(logprob=-0.1420583575963974, rank=1, decoded_token='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n air')}, {637: Logprob(logprob=-0.0024986020289361477, rank=1, decoded_token='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nport')}, {29871: Logprob(logprob=-0.4706132113933563, rank=1, decoded_token='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n ')}, {32001: Logprob(logprob=-0.005849386565387249, rank=1, decoded_token='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[/user]')}, {259: Logprob(logprob=-0.0001110968878492713, rank=1, decoded_token='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  ')}, {32002: Logprob(logprob=-0.0002335037279408425, rank=1, decoded_token='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[assistant]')}], outputs=[CompletionOutput(index=0, text=\"  SELECT icao FROM table_name_74 WHERE airport = 'lilongwe international airport' \", token_ids=[29871, 5097, 474, 1113, 29877, 3895, 1591, 29918, 978, 29918, 29955, 29946, 5754, 4799, 637, 353, 525, 29880, 309, 549, 705, 6121, 4799, 637, 29915, 29871, 32003], cumulative_logprob=-1.4579286568549605, logprobs=[{29871: Logprob(logprob=-1.6093124941107817e-05, rank=1, decoded_token=' ')}, {5097: Logprob(logprob=-0.0011731653939932585, rank=1, decoded_token=' SELECT')}, {474: Logprob(logprob=-1.072112798690796, rank=1, decoded_token=' i')}, {1113: Logprob(logprob=-0.08755894005298615, rank=1, decoded_token='ca')}, {29877: Logprob(logprob=-1.3351351299206726e-05, rank=1, decoded_token='o')}, {3895: Logprob(logprob=-0.00622536102309823, rank=1, decoded_token=' FROM')}, {1591: Logprob(logprob=-0.0008118432597257197, rank=1, decoded_token=' table')}, {29918: Logprob(logprob=-1.4305104514278355e-06, rank=1, decoded_token='_')}, {978: Logprob(logprob=-9.131014667218551e-05, rank=1, decoded_token='name')}, {29918: Logprob(logprob=-6.437280717364047e-06, rank=1, decoded_token='_')}, {29955: Logprob(logprob=-0.0001389883691444993, rank=1, decoded_token='7')}, {29946: Logprob(logprob=-8.642300235806033e-05, rank=1, decoded_token='4')}, {5754: Logprob(logprob=-0.0008311392739415169, rank=1, decoded_token=' WHERE')}, {4799: Logprob(logprob=-0.0040996563620865345, rank=1, decoded_token=' air')}, {637: Logprob(logprob=-4.172238186583854e-05, rank=1, decoded_token='port')}, {353: Logprob(logprob=-0.0031932813581079245, rank=1, decoded_token=' =')}, {525: Logprob(logprob=-0.0027977393474429846, rank=1, decoded_token=\" '\")}, {29880: Logprob(logprob=-0.1968197226524353, rank=1, decoded_token='l')}, {309: Logprob(logprob=-0.0003352795320097357, rank=1, decoded_token='il')}, {549: Logprob(logprob=-4.8636207793606445e-05, rank=1, decoded_token='ong')}, {705: Logprob(logprob=-0.0003909300430677831, rank=1, decoded_token='we')}, {6121: Logprob(logprob=-0.0031704658176749945, rank=1, decoded_token=' international')}, {4799: Logprob(logprob=-0.07165490835905075, rank=1, decoded_token=' air')}, {637: Logprob(logprob=-0.0001012035645544529, rank=1, decoded_token='port')}, {29915: Logprob(logprob=-0.00488386070355773, rank=1, decoded_token=\"'\")}, {29871: Logprob(logprob=-0.0005735897575505078, rank=1, decoded_token=' ')}, {32003: Logprob(logprob=-0.0007503792876377702, rank=1, decoded_token='[/assistant]')}], finish_reason=stop, stop_reason=32003)], finished=True, metrics=RequestMetrics(arrival_time=1717354248.3962412, last_token_time=1717354249.314395, first_scheduled_time=1717354248.4587064, first_token_time=1717354248.5612075, time_in_queue=0.06246519088745117, finished_time=1717354249.3143926), lora_request=LoRARequest(lora_name='sql-lora2', lora_int_id=2, lora_local_path='/home/harpreet_guest2/.cache/huggingface/hub/models--yard1--llama-2-7b-sql-lora-test/snapshots/0dfa347e8877a4d4ed19ee56c140fa518470028c', long_lora_max_len=None))\n",
      "RequestOutput(request_id=3, prompt='[user] Write a SQL query to answer the question based on the table schema.\\n\\n context: CREATE TABLE table_name_11 (nationality VARCHAR, elector VARCHAR)\\n\\n question: When Anchero Pantaleone was the elector what is under nationality? [/user] [assistant]', prompt_token_ids=[1, 32000, 29871, 14350, 263, 3758, 2346, 304, 1234, 278, 1139, 2729, 373, 278, 1591, 10938, 29889, 13, 13, 3030, 29901, 14602, 10911, 1591, 29918, 978, 29918, 29896, 29896, 313, 29876, 1288, 537, 21748, 29892, 3546, 272, 21748, 29897, 13, 13, 1139, 29901, 1932, 530, 305, 1489, 349, 424, 744, 650, 471, 278, 3546, 272, 825, 338, 1090, 4797, 537, 29973, 29871, 32001, 259, 32002], prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=\"  SELECT nationality FROM table_name_11 WHERE elector = 'anchero pantaleone' \", token_ids=[29871, 5097, 4797, 537, 3895, 1591, 29918, 978, 29918, 29896, 29896, 5754, 3546, 272, 353, 525, 14588, 1489, 282, 424, 744, 650, 29915, 29871, 32003], cumulative_logprob=-0.887103857392276, logprobs=None, finish_reason=stop, stop_reason=32003), CompletionOutput(index=1, text=\"  SELECT nationality FROM table_name_11 WHERE elector = 'Anchero Pantaleone' \", token_ids=[29871, 5097, 4797, 537, 3895, 1591, 29918, 978, 29918, 29896, 29896, 5754, 3546, 272, 353, 525, 2744, 305, 1489, 349, 424, 744, 650, 29915, 29871, 32003], cumulative_logprob=-0.9189916047978386, logprobs=None, finish_reason=stop, stop_reason=32003), CompletionOutput(index=2, text=\"  SELECT nationality FROM table_name_11 WHERE elector = 'ANCHERO PANTALEONE' \", token_ids=[29871, 5097, 4797, 537, 3895, 1591, 29918, 978, 29918, 29896, 29896, 5754, 3546, 272, 353, 525, 2190, 3210, 1001, 29949, 349, 2190, 6040, 1307, 12413, 29915, 29871, 32003], cumulative_logprob=-3.722704054903943, logprobs=None, finish_reason=stop, stop_reason=32003)], finished=True, metrics=RequestMetrics(arrival_time=1717354248.3645725, last_token_time=1717354249.3413174, first_scheduled_time=1717354248.364993, first_token_time=1717354248.3952067, time_in_queue=0.00042057037353515625, finished_time=1717354249.3413048), lora_request=LoRARequest(lora_name='sql-lora', lora_int_id=1, lora_local_path='/home/harpreet_guest2/.cache/huggingface/hub/models--yard1--llama-2-7b-sql-lora-test/snapshots/0dfa347e8877a4d4ed19ee56c140fa518470028c', long_lora_max_len=None))\n",
      "RequestOutput(request_id=5, prompt='[user] Write a SQL query to answer the question based on the table schema.\\n\\n context: CREATE TABLE table_name_11 (nationality VARCHAR, elector VARCHAR)\\n\\n question: When Anchero Pantaleone was the elector what is under nationality? [/user] [assistant]', prompt_token_ids=[1, 32000, 29871, 14350, 263, 3758, 2346, 304, 1234, 278, 1139, 2729, 373, 278, 1591, 10938, 29889, 13, 13, 3030, 29901, 14602, 10911, 1591, 29918, 978, 29918, 29896, 29896, 313, 29876, 1288, 537, 21748, 29892, 3546, 272, 21748, 29897, 13, 13, 1139, 29901, 1932, 530, 305, 1489, 349, 424, 744, 650, 471, 278, 3546, 272, 825, 338, 1090, 4797, 537, 29973, 29871, 32001, 259, 32002], prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=\"  SELECT nationality FROM table_name_11 WHERE elector = 'anchero pantaleone' \", token_ids=[29871, 5097, 4797, 537, 3895, 1591, 29918, 978, 29918, 29896, 29896, 5754, 3546, 272, 353, 525, 14588, 1489, 282, 424, 744, 650, 29915, 29871, 32003], cumulative_logprob=-0.887103857392276, logprobs=None, finish_reason=stop, stop_reason=32003), CompletionOutput(index=1, text=\"  SELECT nationality FROM table_name_11 WHERE elector = 'Anchero Pantaleone' \", token_ids=[29871, 5097, 4797, 537, 3895, 1591, 29918, 978, 29918, 29896, 29896, 5754, 3546, 272, 353, 525, 2744, 305, 1489, 349, 424, 744, 650, 29915, 29871, 32003], cumulative_logprob=-0.9189916047978386, logprobs=None, finish_reason=stop, stop_reason=32003), CompletionOutput(index=2, text=\"  SELECT nationality FROM table_name_11 WHERE elector = 'ANCHERO PANTALEONE' \", token_ids=[29871, 5097, 4797, 537, 3895, 1591, 29918, 978, 29918, 29896, 29896, 5754, 3546, 272, 353, 525, 2190, 3210, 1001, 29949, 349, 2190, 6040, 1307, 12413, 29915, 29871, 32003], cumulative_logprob=-3.722704054903943, logprobs=None, finish_reason=stop, stop_reason=32003)], finished=True, metrics=RequestMetrics(arrival_time=1717354248.5619967, last_token_time=1717354249.3413174, first_scheduled_time=1717354248.5623333, first_token_time=1717354248.5923858, time_in_queue=0.00033664703369140625, finished_time=1717354249.341315), lora_request=LoRARequest(lora_name='sql-lora', lora_int_id=1, lora_local_path='/home/harpreet_guest2/.cache/huggingface/hub/models--yard1--llama-2-7b-sql-lora-test/snapshots/0dfa347e8877a4d4ed19ee56c140fa518470028c', long_lora_max_len=None))\n",
      "RequestOutput(request_id=0, prompt='A robot may not injure a human being', prompt_token_ids=[1, 319, 19964, 1122, 451, 10899, 545, 263, 5199, 1641], prompt_logprobs=[None, {319: Logprob(logprob=-4.5557684898376465, rank=8, decoded_token='A'), 917: Logprob(logprob=-2.5245184898376465, rank=1, decoded_token='Tags')}, {19964: Logprob(logprob=-8.996478080749512, rank=1474, decoded_token='robot'), 716: Logprob(logprob=-3.735247850418091, rank=1, decoded_token='new')}, {1122: Logprob(logprob=-4.979100227355957, rank=22, decoded_token='may'), 293: Logprob(logprob=-1.4712878465652466, rank=1, decoded_token='ic')}, {451: Logprob(logprob=-2.097038745880127, rank=2, decoded_token='not'), 367: Logprob(logprob=-1.3626636266708374, rank=1, decoded_token='be')}, {10899: Logprob(logprob=-0.5526919960975647, rank=1, decoded_token='inj')}, {545: Logprob(logprob=-0.0006244616815820336, rank=1, decoded_token='ure')}, {263: Logprob(logprob=-0.027345167472958565, rank=1, decoded_token='a')}, {5199: Logprob(logprob=-0.016767755150794983, rank=1, decoded_token='human')}, {1641: Logprob(logprob=-0.06561322510242462, rank=1, decoded_token='being')}], outputs=[CompletionOutput(index=0, text=' or, through inaction, allow a human being to come to harm.\\nA robot must obey the orders given it by human beings except where such orders would conflict with the First Law.\\nA robot must protect its own existence as long as such protection does not conflict with the First or Second Law.\\nThe Three Laws of Robotics were created by Isaac Asimov in 1942. They are the foundation of robotics and artificial intelligence.\\nThe Three Laws of Robotics are the foundation of robotics and artificial intelligence. They were created by Isaac Asimov in 194', token_ids=[470, 29892, 1549, 297, 2467, 29892, 2758, 263, 5199, 1641, 304, 2041, 304, 10311, 29889, 13, 29909, 19964, 1818, 26449, 278, 11299, 2183, 372, 491, 5199, 367, 886, 5174, 988, 1316, 11299, 723, 14529, 411, 278, 3824, 7927, 29889, 13, 29909, 19964, 1818, 12566, 967, 1914, 10379, 408, 1472, 408, 1316, 13047, 947, 451, 14529, 411, 278, 3824, 470, 6440, 7927, 29889, 13, 1576, 12753, 997, 5652, 310, 6417, 327, 1199, 892, 2825, 491, 28156, 1094, 326, 586, 297, 29871, 29896, 29929, 29946, 29906, 29889, 2688, 526, 278, 22778, 310, 19964, 1199, 322, 23116, 21082, 29889, 13, 1576, 12753, 997, 5652, 310, 6417, 327, 1199, 526, 278, 22778, 310, 19964, 1199, 322, 23116, 21082, 29889, 2688, 892, 2825, 491, 28156, 1094, 326, 586, 297, 29871, 29896, 29929, 29946], cumulative_logprob=-42.51186228008737, logprobs=[{470: Logprob(logprob=-0.7071324586868286, rank=1, decoded_token=' or')}, {29892: Logprob(logprob=-0.0695793554186821, rank=1, decoded_token=',')}, {1549: Logprob(logprob=-0.0565168559551239, rank=1, decoded_token=' through')}, {297: Logprob(logprob=-0.006628549657762051, rank=1, decoded_token=' in')}, {2467: Logprob(logprob=-0.0024920618161559105, rank=1, decoded_token='action')}, {29892: Logprob(logprob=-0.009607388637959957, rank=1, decoded_token=',')}, {2758: Logprob(logprob=-0.013026613742113113, rank=1, decoded_token=' allow')}, {263: Logprob(logprob=-0.0036942586302757263, rank=1, decoded_token=' a')}, {5199: Logprob(logprob=-0.014817187562584877, rank=1, decoded_token=' human')}, {1641: Logprob(logprob=-0.01966959983110428, rank=1, decoded_token=' being')}, {304: Logprob(logprob=-0.0053190141916275024, rank=1, decoded_token=' to')}, {2041: Logprob(logprob=-0.002538674511015415, rank=1, decoded_token=' come')}, {304: Logprob(logprob=-0.004207449499517679, rank=1, decoded_token=' to')}, {10311: Logprob(logprob=-0.019833462312817574, rank=1, decoded_token=' harm')}, {29889: Logprob(logprob=-0.3278198838233948, rank=1, decoded_token='.')}, {13: Logprob(logprob=-0.7407960891723633, rank=1, decoded_token='\\n')}, {29909: Logprob(logprob=-1.0346400737762451, rank=1, decoded_token='A')}, {19964: Logprob(logprob=-0.009795219637453556, rank=1, decoded_token=' robot')}, {1818: Logprob(logprob=-0.06791557371616364, rank=1, decoded_token=' must')}, {26449: Logprob(logprob=-0.10044898092746735, rank=1, decoded_token=' obey')}, {278: Logprob(logprob=-0.6801989674568176, rank=1, decoded_token=' the')}, {11299: Logprob(logprob=-0.006660996470600367, rank=1, decoded_token=' orders')}, {2183: Logprob(logprob=-0.0027608871459960938, rank=1, decoded_token=' given')}, {372: Logprob(logprob=-0.4183964729309082, rank=1, decoded_token=' it')}, {491: Logprob(logprob=-0.014296486973762512, rank=1, decoded_token=' by')}, {5199: Logprob(logprob=-0.020199310034513474, rank=1, decoded_token=' human')}, {367: Logprob(logprob=-0.06251794844865799, rank=1, decoded_token=' be')}, {886: Logprob(logprob=-1.7165990357170813e-05, rank=1, decoded_token='ings')}, {5174: Logprob(logprob=-0.38280394673347473, rank=1, decoded_token=' except')}, {988: Logprob(logprob=-0.009292683564126492, rank=1, decoded_token=' where')}, {1316: Logprob(logprob=-0.0002479245886206627, rank=1, decoded_token=' such')}, {11299: Logprob(logprob=-0.009298352524638176, rank=1, decoded_token=' orders')}, {723: Logprob(logprob=-0.003799482947215438, rank=1, decoded_token=' would')}, {14529: Logprob(logprob=-0.0011625682236626744, rank=1, decoded_token=' conflict')}, {411: Logprob(logprob=-0.0015200738562271, rank=1, decoded_token=' with')}, {278: Logprob(logprob=-0.013279564678668976, rank=1, decoded_token=' the')}, {3824: Logprob(logprob=-0.05764719843864441, rank=1, decoded_token=' First')}, {7927: Logprob(logprob=-0.0569356344640255, rank=1, decoded_token=' Law')}, {29889: Logprob(logprob=-0.06237683817744255, rank=1, decoded_token='.')}, {13: Logprob(logprob=-0.023910852149128914, rank=1, decoded_token='\\n')}, {29909: Logprob(logprob=-0.35536766052246094, rank=1, decoded_token='A')}, {19964: Logprob(logprob=-0.004390006884932518, rank=1, decoded_token=' robot')}, {1818: Logprob(logprob=-0.03327304869890213, rank=1, decoded_token=' must')}, {12566: Logprob(logprob=-0.005752793047577143, rank=1, decoded_token=' protect')}, {967: Logprob(logprob=-0.0027308098506182432, rank=1, decoded_token=' its')}, {1914: Logprob(logprob=-0.005350555293262005, rank=1, decoded_token=' own')}, {10379: Logprob(logprob=-0.0012635351158678532, rank=1, decoded_token=' existence')}, {408: Logprob(logprob=-0.009133345447480679, rank=1, decoded_token=' as')}, {1472: Logprob(logprob=-6.294052582234144e-05, rank=1, decoded_token=' long')}, {408: Logprob(logprob=-0.001043133088387549, rank=1, decoded_token=' as')}, {1316: Logprob(logprob=-0.00043156370520591736, rank=1, decoded_token=' such')}, {13047: Logprob(logprob=-0.001057184999808669, rank=1, decoded_token=' protection')}, {947: Logprob(logprob=-0.0021620250772684813, rank=1, decoded_token=' does')}, {451: Logprob(logprob=-0.0010886224918067455, rank=1, decoded_token=' not')}, {14529: Logprob(logprob=-0.00662073353305459, rank=1, decoded_token=' conflict')}, {411: Logprob(logprob=-0.0006914132391102612, rank=1, decoded_token=' with')}, {278: Logprob(logprob=-0.007761795073747635, rank=1, decoded_token=' the')}, {3824: Logprob(logprob=-0.0033597471192479134, rank=1, decoded_token=' First')}, {470: Logprob(logprob=-0.011276324279606342, rank=1, decoded_token=' or')}, {6440: Logprob(logprob=-0.005028102546930313, rank=1, decoded_token=' Second')}, {7927: Logprob(logprob=-0.07395392656326294, rank=1, decoded_token=' Law')}, {29889: Logprob(logprob=-0.05316426232457161, rank=1, decoded_token='.')}, {13: Logprob(logprob=-0.05270714685320854, rank=1, decoded_token='\\n')}, {1576: Logprob(logprob=-1.9789133071899414, rank=1, decoded_token='The')}, {12753: Logprob(logprob=-1.7799919843673706, rank=1, decoded_token=' Three')}, {997: Logprob(logprob=-0.01616608165204525, rank=1, decoded_token=' La')}, {5652: Logprob(logprob=-1.2516897186287679e-05, rank=1, decoded_token='ws')}, {310: Logprob(logprob=-0.4606093168258667, rank=1, decoded_token=' of')}, {6417: Logprob(logprob=-0.026053905487060547, rank=1, decoded_token=' Rob')}, {327: Logprob(logprob=-0.015243579633533955, rank=1, decoded_token='ot')}, {1199: Logprob(logprob=-0.0006384003208950162, rank=1, decoded_token='ics')}, {892: Logprob(logprob=-1.1325634717941284, rank=1, decoded_token=' were')}, {2825: Logprob(logprob=-1.822002649307251, rank=1, decoded_token=' created')}, {491: Logprob(logprob=-0.2594166100025177, rank=1, decoded_token=' by')}, {28156: Logprob(logprob=-0.7868831157684326, rank=1, decoded_token=' Isaac')}, {1094: Logprob(logprob=-0.003043664153665304, rank=1, decoded_token=' As')}, {326: Logprob(logprob=-0.0017087158048525453, rank=1, decoded_token='im')}, {586: Logprob(logprob=-0.0005137792322784662, rank=1, decoded_token='ov')}, {297: Logprob(logprob=-0.770645797252655, rank=1, decoded_token=' in')}, {29871: Logprob(logprob=-0.7651343941688538, rank=1, decoded_token=' ')}, {29896: Logprob(logprob=-0.0010563514661043882, rank=1, decoded_token='1')}, {29929: Logprob(logprob=-0.00018320789968129247, rank=1, decoded_token='9')}, {29946: Logprob(logprob=-0.011811322532594204, rank=1, decoded_token='4')}, {29906: Logprob(logprob=-0.022102804854512215, rank=1, decoded_token='2')}, {29889: Logprob(logprob=-1.1060770750045776, rank=1, decoded_token='.')}, {2688: Logprob(logprob=-1.4170829057693481, rank=1, decoded_token=' They')}, {526: Logprob(logprob=-1.2098824977874756, rank=1, decoded_token=' are')}, {278: Logprob(logprob=-1.6315357685089111, rank=1, decoded_token=' the')}, {22778: Logprob(logprob=-1.7198727130889893, rank=1, decoded_token=' foundation')}, {310: Logprob(logprob=-0.400881826877594, rank=1, decoded_token=' of')}, {19964: Logprob(logprob=-1.2221651077270508, rank=1, decoded_token=' robot')}, {1199: Logprob(logprob=-0.8893821835517883, rank=1, decoded_token='ics')}, {322: Logprob(logprob=-1.3666355609893799, rank=1, decoded_token=' and')}, {23116: Logprob(logprob=-1.4927663803100586, rank=1, decoded_token=' artificial')}, {21082: Logprob(logprob=-0.011553286574780941, rank=1, decoded_token=' intelligence')}, {29889: Logprob(logprob=-0.936883270740509, rank=1, decoded_token='.')}, {13: Logprob(logprob=-1.2797105312347412, rank=1, decoded_token='\\n')}, {1576: Logprob(logprob=-1.422471523284912, rank=1, decoded_token='The')}, {12753: Logprob(logprob=-1.3517884016036987, rank=1, decoded_token=' Three')}, {997: Logprob(logprob=-0.008786113932728767, rank=1, decoded_token=' La')}, {5652: Logprob(logprob=-5.364403477869928e-06, rank=1, decoded_token='ws')}, {310: Logprob(logprob=-0.770526647567749, rank=1, decoded_token=' of')}, {6417: Logprob(logprob=-0.02055860124528408, rank=1, decoded_token=' Rob')}, {327: Logprob(logprob=-0.0012894895626232028, rank=1, decoded_token='ot')}, {1199: Logprob(logprob=-0.0004362108593340963, rank=1, decoded_token='ics')}, {526: Logprob(logprob=-1.1972577571868896, rank=1, decoded_token=' are')}, {278: Logprob(logprob=-2.0295071601867676, rank=1, decoded_token=' the')}, {22778: Logprob(logprob=-1.6423799991607666, rank=1, decoded_token=' foundation')}, {310: Logprob(logprob=-0.21224558353424072, rank=1, decoded_token=' of')}, {19964: Logprob(logprob=-0.42446088790893555, rank=1, decoded_token=' robot')}, {1199: Logprob(logprob=-0.2259061485528946, rank=1, decoded_token='ics')}, {322: Logprob(logprob=-0.18002019822597504, rank=1, decoded_token=' and')}, {23116: Logprob(logprob=-0.17628633975982666, rank=1, decoded_token=' artificial')}, {21082: Logprob(logprob=-0.008804547600448132, rank=1, decoded_token=' intelligence')}, {29889: Logprob(logprob=-0.23802471160888672, rank=1, decoded_token='.')}, {2688: Logprob(logprob=-0.8600689768791199, rank=1, decoded_token=' They')}, {892: Logprob(logprob=-0.7509920597076416, rank=1, decoded_token=' were')}, {2825: Logprob(logprob=-0.40819239616394043, rank=1, decoded_token=' created')}, {491: Logprob(logprob=-0.08251062780618668, rank=1, decoded_token=' by')}, {28156: Logprob(logprob=-0.08961062878370285, rank=1, decoded_token=' Isaac')}, {1094: Logprob(logprob=-0.0008559139096178114, rank=1, decoded_token=' As')}, {326: Logprob(logprob=-0.00047600860125385225, rank=1, decoded_token='im')}, {586: Logprob(logprob=-0.0002302858338225633, rank=1, decoded_token='ov')}, {297: Logprob(logprob=-0.0867311879992485, rank=1, decoded_token=' in')}, {29871: Logprob(logprob=-0.09981191158294678, rank=1, decoded_token=' ')}, {29896: Logprob(logprob=-0.00013362467871047556, rank=1, decoded_token='1')}, {29929: Logprob(logprob=-6.151010165922344e-05, rank=1, decoded_token='9')}, {29946: Logprob(logprob=-0.0008345934911631048, rank=1, decoded_token='4')}], finish_reason=length, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1717354248.1476576, last_token_time=1717354251.5775058, first_scheduled_time=1717354248.151556, first_token_time=1717354248.176249, time_in_queue=0.0038983821868896484, finished_time=1717354251.5774968), lora_request=None)\n",
      "RequestOutput(request_id=1, prompt='To be or not to be,', prompt_token_ids=[1, 1763, 367, 470, 451, 304, 367, 29892], prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' that is the question.\\nI am not sure what I would do if I had to make a decision to live or die.\\nI would probably die, just to be sure.\\nI think I would die, because if I were to live, I would not be able to live.\\nIf I were dead, I could live, but I would not be happy.\\nIf I had to choose between living and dying, I think that I would choose to live, because if I did die, I would have no regrets.\\nI would also be able to see my loved ones again and be with them forever.\\n', token_ids=[393, 338, 278, 1139, 29889, 13, 29902, 626, 451, 1854, 825, 306, 723, 437, 565, 306, 750, 304, 1207, 263, 10608, 304, 5735, 470, 762, 29889, 13, 29902, 723, 3117, 762, 29892, 925, 304, 367, 1854, 29889, 13, 29902, 1348, 306, 723, 762, 29892, 1363, 565, 306, 892, 304, 5735, 29892, 306, 723, 451, 367, 2221, 304, 5735, 29889, 13, 3644, 306, 892, 7123, 29892, 306, 1033, 5735, 29892, 541, 306, 723, 451, 367, 9796, 29889, 13, 3644, 306, 750, 304, 6755, 1546, 8471, 322, 27116, 29892, 306, 1348, 393, 306, 723, 6755, 304, 5735, 29892, 1363, 565, 306, 1258, 762, 29892, 306, 723, 505, 694, 1072, 27487, 29889, 13, 29902, 723, 884, 367, 2221, 304, 1074, 590, 18012, 6743, 1449, 322, 367, 411, 963, 22296, 29889, 13], cumulative_logprob=-111.53696845662122, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1717354248.1767263, last_token_time=1717354251.5775058, first_scheduled_time=1717354248.1769104, first_token_time=1717354248.2304647, time_in_queue=0.00018405914306640625, finished_time=1717354251.5775037), lora_request=None)\n"
     ]
    }
   ],
   "source": [
    "test = main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: A robot may not injure a human being\n",
      "Output:  or, through inaction, allow a human being to come to harm.\n",
      "A robot must obey the orders given it by human beings except where such orders would conflict with the First Law.\n",
      "A robot must protect its own existence as long as such protection does not conflict with the First or Second Law.\n",
      "The Three Laws of Robotics were created by Isaac Asimov in 1942. They are the foundation of robotics and artificial intelligence.\n",
      "The Three Laws of Robotics are the foundation of robotics and artificial intelligence. They were created by Isaac Asimov in 194\n",
      "\n",
      "Prompt: To be or not to be,\n",
      "Output:  that is the question.\n",
      "I am not sure what I would do if I had to make a decision to live or die.\n",
      "I would probably die, just to be sure.\n",
      "I think I would die, because if I were to live, I would not be able to live.\n",
      "If I were dead, I could live, but I would not be happy.\n",
      "If I had to choose between living and dying, I think that I would choose to live, because if I did die, I would have no regrets.\n",
      "I would also be able to see my loved ones again and be with them forever.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for elem in test:\n",
    "    print(\"Prompt:\", elem.prompt)\n",
    "    for output in elem.outputs:\n",
    "        print(\"Output:\", output.text)\n",
    "    print()  # empty line to separate outputs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
